---
title: Performance Metrics Comparison
output:
  html_document: 
    toc: true
    toc_depth: 3
  word_document:
    toc: true
    toc_depth: 3
---

```{r Setup, include=FALSE}
rm(list=ls())
library(dplyr)
library(tidyr)
library(broom)
library(tibble)
library(modelsummary)
library(tidyverse)
library(ggpubr)
library(viridis)
library(knitr)
load(file="/Users/xindieh/Documents/3_Academics/Projects/8_SelectionBias/simulation/simstudy/bias_factor_df.RData")
## auxiliary function
f <- function(data=dfreg, parname="betaO1", trimpct=0) {
  res <- list()
  ## select interested parameter
  if (trimpct==0) {
    df <- data %>% dplyr::filter(parameter==parname)
  } else {
    ## remove outliers for each method
    df <- data %>% 
      dplyr::filter(parameter==parname) %>% 
      dplyr::group_by(method) %>% 
      dplyr::filter(AE<=quantile(AE, 1-trimpct)) %>% 
      dplyr::ungroup()
  } 
  ## overall comparison
  cnames1 <- c("Method", "MAE", "SD(AE)", "RMSE", "Ture in 95%CI", "Mean Width of 95%CI")
  main1 <- paste0("Performance on ", parname, " : across Selection-Outcome Specifications")
  res$overall <- df %>%
    dplyr::group_by(method) %>%
    dplyr::summarise(MAE = mean(AE), SDAE = sd(AE), RMSE = sqrt(mean(SE)), 
                     inCI = mean(inCI), MlenCI = mean(lenCI), .groups='drop') %>% 
    kable(caption=main1, col.names=cnames1, 
          format.args=list(digits=3, width=6, trim=TRUE, scientific=FALSE))
  ## by marginal distributions
  if (grepl(x=parname, pattern="betaS")) {
    methodnames <- c("Heckit", "Copula")
  } else {
    methodnames <- c("OLS", "Heckit", "Copula")
  }
  cnames2 <- c("Selection Margin", "Outcome Margin", 
               paste0("MAE ", methodnames), 
               paste0("SD(AE) ", methodnames),
               paste0("RMSE ", methodnames),
               paste0("Ture in 95%CI ", methodnames),
               paste0("Mean Width of 95%CI ", methodnames))
  main2 <- paste0("Performance on ", parname, " : by Selection-Outcome Specifications")
  res$bymargin <- df %>%
    dplyr::group_by(select_dist, outcome_dist, method) %>%
    dplyr::summarise(MAE = mean(AE), SDAE = sd(AE), RMSE = sqrt(mean(SE)), 
                     inCI = mean(inCI), MlenCI = mean(lenCI), .groups='drop') %>%
    dplyr::arrange(desc(select_dist), outcome_dist, desc(method)) %>%
    tidyr::pivot_wider(id_cols=c(select_dist,outcome_dist), 
                       names_from=method,
                       values_from=c(MAE,SDAE,RMSE,inCI,MlenCI)) %>% 
    dplyr::ungroup() %>% 
    kable(caption=main2, col.names=cnames2, 
          format.args=list(digits=3, width=6, trim=TRUE, scientific=FALSE))
  
  return(res)
}
```

## No Trimming

Summary: 

- The performance metrics in this section are caluclated based on all simulations, including those with extreme biases.

- The huge error in OLS case is solely driven by estimates in Probit-Probit case. 

- A look at what these extreme estimates have in common:
  - the table lists estimations with $Absolute Error (\hat{\beta}^{OLS}_{O1}) > 5$.
  - mostly low visibility + moderate to high dependence + Poisson X1.
  
```{r, echo=FALSE}
check <- dfreg %>% 
  dplyr::filter(parameter=="betaO1", method=="OLS", select_dist=="Probit", outcome_dist=="Probit") %>% 
  dplyr::select(c(estimate, se, lb, ub, pval, dependence, selectivity, x_dist, seed, pctobs, AE, inCI, lenCI)) %>% 
  dplyr::filter(AE>5) %>% 
  kable(format.args=list(digits=3, width=6, trim=TRUE))
check
```


```{r trim0, echo=FALSE, results='asis'}
parnames <- c("betaO0", "betaO1", "betaO2", "betaS0", "betaS1", "betaS2", "betaS3")
parlabel <- c("outcome equation, intercept", "outcome equation, endogenous var X1 coef",
              "outcome equation, exogenous var X2 coef", 
              "selection equation, intercept", "selection equation, endogenous var X1 coef",
              "selection equation, exogenous var X2 coef", "selection equation, exclusion restriction var X3 coef")
res0 <- list()
for (i in 1:(length(parnames))) {
  res0[[i]] <- f(data=dfreg, parname=parnames[[i]], trimpct=0)
  cat(paste0("\n\n\n####  ", parnames[[i]], "  :  ", parlabel[[i]], "\n"))
  print(res0[[i]][[1]])
  cat("\n")
  print(res0[[i]][[2]])
  cat("\n")
}
```



## Trimming off 0.3%

Summary: 

- The metrics in this section are calculated by excluding simulations with the most extreme 0.3% Absolute Errors. 


```{r trim003, echo=FALSE, results='asis'}
savedir <- "/Users/xindieh/Documents/3_Academics/Projects/8_SelectionBias/simulation/simstudy/"
res1 <- list()
for (i in 1:(length(parnames))) {
  res1[[i]] <- f(data=dfreg, parname=parnames[[i]], trimpct=0.003)
  cat(paste0("\n\n\n####  ", parnames[[i]], "  :  ", parlabel[[i]], "\n"))
  print(res1[[i]][[1]])
  write.table(res1[[i]][[1]], file=paste0(savedir,"metrics_",parnames[[i]],".csv"), 
              sep=", ", row.names = FALSE)
  cat("\n")
  print(res1[[i]][[2]])
  write.table(res1[[i]][[2]], file=paste0(savedir,"metrics_by_",parnames[[i]],".csv"),
              sep=", ", row.names = FALSE)
  cat("\n")
}
```
